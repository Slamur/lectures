# Dynamic Array

The simplest dynamic data structure is an array with the ability to add/remove elements (and, accordingly, change its own size).

Let's consider approaches to efficiently implement such a structure.

## Important Operations

First, let's fix which operations we want to perform **efficiently**:

- get the value **by index**.
- add an element **to the end of the array**;
- remove an element **from the end of the array**.

Ideally, these two operations should be performed in $O(1)$ (at least _amortized_).

Yes, our array should also be able to insert/remove values **from arbitrary positions**, but there are no explicit efficiency requirements imposed on this.

## Implementation on an Array

In the simplest case, a dynamic array is built on the basis of a **fixed-size array**.

Let's introduce the following notations:

- $S$ (size) - the size of the dynamic array as a structure for the external user;
- $C$ (capacity) - the fixed size of the internal array.

In this case, the overall logic of the _important operations_ is expressed quite simply:

- when retrieving an element by index:
  - simply access the specified index in the internal array in $O(1)$.
- when adding an element:
  - if $S = C$, then the internal array is expanded;
    - memory is allocated for a new array;
    - all elements are moved to the new array.
  - if adding at position $P$: the suffix of the array is shifted in $O(S - P)$;
  - the new element simply takes its place in $O(1)$.
- when removing an element:
  - if removing from position $P$: the suffix of the array is shifted in $O(S - P)$;
  - (optionally) under certain conditions, the internal array may be reduced to save memory.

It is obvious that the only "heavy" operation is the expansion of the internal array. Let's consider two approaches to implementing this part of the functionality.

### Additive Approach

In this case, the array is expanded by an additional $D$ cells ($D$ is fixed).

Without loss of generality, let's assume that $N = D \cdot K + 1$ and we start with an empty array.

In this case, the total number of **element moves when adding an element to the end** will be $T(D, K) = D + 2 \cdot D + \dots + K \cdot D$.

By the formula for the arithmetic progression, $T(D, K) = \frac{(K + 1) \cdot K \cdot D}{2} \sim \frac{K^2 \cdot D}{2}$.

Considering that $K = \frac{N}{D}$, we get $T(N, D) = O(\frac{N^2}{D})$.

From this, the amortized complexity of adding an element turns out to be $O(\frac{N}{D})$ - linear in $N$, but tending towards a constant as $D$ approaches $N$:

- either $D$ is too small and there are too many moves;
- or $D$ is large enough, resulting in a lot of excess memory being allocated.

### Multiplicative Approach

Let's try to expand the array by multiplying the current size by a real coefficient $W = (\lambda + 1)$:

- at $\lambda = 1$, we get the previous size multiplied by $2$.

Without loss of generality, let's assume that $N = W^K + 1$ and we start with an array of size $1$.

In this case, the total number of **element moves when adding an element to the end** will be $T(W, K) = (W^0 + W^1 + \dots + W^K)$.

By the formula for the geometric progression, $T(W, K) = \frac{W^{K + 1} - 1}{W - 1} \sim \frac{W^{K + 1}}{\lambda}$.

Considering that $W^K \sim N$, we get $T(N, \lambda) \sim \frac{N \cdot (\lambda + 1)}{\lambda} = N + \frac{N}{\lambda}$.

From this, the amortized complexity of adding an element turns out to be $O(1 + \frac{1}{\lambda})$ - constant for a fixed $\lambda$:

- at $\lambda = 1$, we get an _average_ of $2$ move operations per element.

Note that as $\lambda$ increases, the number of move operations will decrease, but similarly to the case of a large $D$, the size of the "excess" allocated memory will increase.