# Operations on Asymptotics

## Addition

Let the algorithm $T$ consist of sequentially executing the algorithms $F$, $G$, $H$.

	Algorithm T(N) {
		F(N);
		G(N);
		H(N);
	}

In this case, the asymptotic complexity $O(T(N))$ will be equal to the **sum** of the asymptotic complexities $O(F(N) + G(N) + H(N))$.

## Multiplication

Let the algorithm $T$ consist of $M$ iterations of executing the algorithm $F$.

	Algorithm T(N) {
		Repeat M times {
			F(N);
		}
	}

In this case, the asymptotic complexity $O(T(N))$ will be equal to the **product** of the asymptotic complexity $O(M \cdot F(N))$.

# Commonly Used Asymptotics

Let's consider functions that are most commonly used when describing the asymptotic complexity of a given algorithm.

For each, we will provide an upper **estimate** $N$ at which the function "fits into a second" (value of the order of $10^7 - 10^8$).

## O(1) - constant

Any operation that is considered independent of the size of the parameters (close to a fixed number of clock cycles):

### Examples

- Arithmetic operations;
- Conditional statement;
- Accessing an array element;
- And so on.

## O(logN) - logarithm

### Examples

- Number of digits or bits in the number $N$;
- Number of integer divisions of the number $N$ by $A$ down to $0$;

Pseudocode:
<details> <summary> </summary>

div - integer division.

	Algorithm F(N, A) {
		While (N > 0) {
			N = N div A;
		}
	}

</details>

- Division and calculation of the remainder ("fast" logarithm).
- Addition and subtraction of [long numbers](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic).

### Upper estimate N

By definition, for any integer representable in a $64$-bit type, the logarithm does not exceed $64$.

Therefore, usually the logarithm is not a limitation on $N$, but it is important to consider it when multiplying asymptotics.

## O(N) - linear

### Examples

- Number of elements in an array / characters in a string of length $N$;
- Number of integers from $1$ to $N$.
- Sum of a decreasing geometric progression $\frac{N}{1} + \frac{N}{B} + \dots + \frac{N}{B^K}$ when $B > 1$.

### Upper estimate N

If we specifically talk about $O(N)$ operations, then usually $N \le 10^6 - 10^7$.

On the other hand, often $N$ is the total size of the input data.

Since you must be able to read / generate all input data in $O(N)$, any processing of $O(1)$ for the "input size" should also fit.

## O(N^2) - quadratic

### Examples

- Sum of integers from $1$ to $N$.
- Number of pairs of natural numbers $(i, j)$, where $1 \le i \le j \le N$.
- Number of **substrings** of a string of length $N$ (since each substring is defined by a pair of indices $1 \le i \le j \le N$).

### Upper estimate N

Formally, $N \le 10^4$.

But it greatly depends on the asymptotic complexity and the constant of individual operations.

Most often, $O(N^2)$ can be encountered when $N \le 2 \cdot 10^3 - 5 \cdot 10^3$.

## Conclusions

- Sequential execution of algorithms leads to the addition of their asymptotic complexities.
- From this, it follows that repeating the same algorithm leads to multiplying its asymptotic complexity by the number of repetitions.
- There are simple examples of asymptotic complexities from which the asymptotic complexity of most algorithms can be composed.
- It is important to understand that the list provided is far from exhaustive, but it covers the most common asymptotic complexities used during this course.